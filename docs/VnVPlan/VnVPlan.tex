\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt}

\begin{document}

\title{System Verification and Validation Plan for \progname{}}
\author{\authname}
\date{\today}

\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Date 1 & 1.0 & Notes\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l}
  \toprule
  \textbf{symbol} & \textbf{description}\\
  \midrule
  PR & Pull request\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
  \citep{SRS} tables, if appropriate}

\wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

\noindent This Verification and Validation (VnV) plan outlines the strategies that 
will be employed to ensure that \progname{} meets its specified requirements
and functions correctly. This document covers the methods and criteria used
to confirm that the software is built in a way that satisfies the outlined
requirements, and that fulfills its intended purpose.

% \wss{provide an introductory blurb and roadmap of the
%   Verification and Validation plan}

\section{General Information}

\subsection{Summary}

% \wss{Say what software is being tested.  Give its name and a brief overview of
  % its general functions.}

\progname{} is a note-taking application designed to convert keyboard input into
visual diagrams and structured graphical representations. In addition to basic
note-taking software features, such as creating, opening, editing, saving, and
deleting notes, \progname{} aims to enhance user productivity by allowing users
to quickly generate visual content from text input. 

\vspace{1em}

\noindent Each note will be represented as a scrollable canvas where users can insert,
edit, or delete text within text boxes. \progname{} will also support the insertion of
predefined geometric shapes, as well as custom workflow shapes that users can
define. The Verification and Validation (VnV) process will focus on ensuring
that these core features function as specified, user interface elements work
efficiently and intuitively, and that user interactions meet the outlined 
requirements.
noindent
\subsection{Objectives}

% \wss{State what is intended to be accomplished.  The objective will be around
%   the qualities that are most important for your project.  You might have
%   something like: ``build confidence in the software correctness,''
%   ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
%   just those that are most important.}

% \wss{You should also list the objectives that are out of scope.  You don't have 
% the resources to do everything, so what will you be leaving out.  For instance, 
% if you are not going to verify the quality of usability, state this.  It is also 
% worthwhile to justify why the objectives are left out.}

% \wss{The objectives are important because they highlight that you are aware of 
% limitations in your resources for verification and validation.  You can't do everything, 
% so what are you going to prioritize?  As an example, if your system depends on an 
% external library, you can explicitly state that you will assume that external library 
% has already been verified by its implementation team.}

The primary objective of this Verification and Validation (VnV) plan is to
ensure that \progname{} meets the specified requirements outlined in the Software
Requirements Specification (SRS) document. This process aims to build confidence
that the system functions correctly, is reliable, and provides a user-friendly
experience. The objectives also aims to test activities that mitigate the risks
identified in the Hazard Analysis document. This will ensure that the system is
robust and can handle potential failure scenarios gracefully.

\vspace{1em}

\noindent Specifically, the VnV plan focuses on the following objectives:
\begin{itemize}
  \item \textbf{Document Management Correctness}: Verifying that file related operations,
  such as creating, opening, saving, deleting and file organization of notes, 
  function as intended without data loss or corruption.
  \item \textbf{Note and Diagram Manipulation Correctness}: Ensuring that editing features
  related to note-taking and diagram creation work as specified. This includes
  textbox and geometric diagram manipulation (inserting, resizing, moving, and 
  deleting), as well as text formatting capabilities (bolding, italicizing,
  underlining, etc).
  \item \textbf{Data Integrity}: Confirming that all user inputs and modifications are saved
  accurately and can be retrieved without corruption or loss.
  \item \textbf{Performance and Responisiveness}: Verifying that the user interactions occur 
  within acceptable time frames, ensuring a smooth and efficient user experience.
  \item \textbf{System reliability}: Confirming that the system operates consistently in
  different sessions without crashes or unexpected behavior.

\end{itemize}
\vspace{1em}

\noindent Objectives that are out of scope for this VnV plan include:
\begin{itemize}
  \item \textbf{Formal Usability Testing}: Due to resource constraints, formal usability testing
  with a large user base will not be conducted. Instead, an informal usability test from 
  team members within the development team will be used as these members fall into
  the target user demographic.
  \item \textbf{Security Testing}: Given the nature of \progname{} being a local note-taking
noindent  application, security testing is not prioritized in this VnV plan. The focus
  will be on functionality and usability rather than security vulnerabilities.
  \item \textbf{External Library Verification}: Any third-party libraries or frameworks used
  for rendering or GUI components will be assumed to have been verified by their
  respective development teams. 
\end{itemize}  

\subsection{Extras}

% \wss{Summarize the extras (if any) that were tackled by this project.  Extras
% can include usability testing, code walkthroughs, user documentation, formal
% proof, GenderMag personas, Design Thinking, etc.  Extras should have already
% been approved by the course instructor as included in your problem statement.
% You can use a pull request to update your extras (in TeamComposition.csv or
% Repos.csv) if your plan changes as a result of the VnV planning exercise.}

\noindent As part of the VnV prcess for \progname{}, the project will include the 
following two approved extras: Usability Testing and a User Manual.

\begin{itemize}
  \item \textbf{Usability Testing}: An informal, but structured test will be 
  conducted with team members who fall within the target user demographic. It 
  will be done to evaluate the intuitiveness and efficiency of the user interface
  and workflows of \progname{}. Methods will be made as systematic and objective 
  as possible, to help closely identify areas for improvement. Participants will
  complete a set of predefined tasks (e.g. creating and organizing notes, inserting
  and editing diagrams, etc.) while their performance and completion times are
  recorded (although participants are team members, they will not be involved in
  the elicitation of the specific details of the set tasks to avoid bias).
  Afterwards, participants will be given an opportunity to provide feedback on 
  their experience, including any difficulties encountered and suggestions for 
  enhancements.
  \item \textbf{User Manual}: A comprehensive user manual will be created to
  guide new users through the features and functionalities of \progname{}. The
  manual will include step-by-step instructions for creating, editing, and 
  managing notes and diagrams. It will also have explanations of various tools
  and keyboard shortcuts. The user manual will serve as both a user support 
  resource and a validation tool to ensure that all features are accessible and
  understandable to users.
\end{itemize}

\subsection{Relevant Documentation}

% \wss{Reference relevant documentation.  This will definitely include your SRS
%   and your other project documents (design documents, like MG, MIS, etc).  You
%   can include these even before they are written, since by the time the project
%   is done, they will be written.  You can create BibTeX entries for your
%   documents and within those entries include a hyperlink to the documents.}

% \citet{SRS}

% \wss{Don't just list the other documents.  You should explain why they are relevant and 
% how they relate to your VnV efforts.}

\noindent The following documents are relevant and closely linked to the Verification
and Validation (VnV) document. These documents define and guide the development of 
\progname{}:

\begin{itemize}
  \item \textbf{Software Requirements Specification (SRS)}: 
  The SRS establishes the functional and non-functional requirements for \progname{},
  forming the foundation for the VnV activities. Sections such as the Functional 
  Overview (G.4), High-Level Usage Scenarios (G.5), and System Components (S.1) were
  referenced to ensure the testing objectives and procedures align with \progname{}'s
  specified features and structure.
  \item \textbf{Problem Statement}:
  The Problem Statement outlines the project's goals and motivations. It was referenced
  to ensure that the VnV objectives align with the intended purpose of \progname{}. The 
  Extras section of the Problem Statement was also used to identify the approved extras 
  for the project, which are included in this VnV plan.
  \item \textbf{Design Documents (MG, MIS)}:
  The Module Guide (MG) and Module Interface Specification (MIS) will provide detailed
  descriptions of the system's architecture and module interactions. During develpoment
  phases of the project, as well as development of unit and integration testing, the 
  MG and MIS will be referenced to ensure that each component performs as intended and
  that the interactions between modules function correctly.
  \item \textbf{Hazard Analysis}:
  The Hazard Analysis document identifies potential risks and failure areas within
  \progname{}. This document was referenced to inform the VnV plan's objectives. Key 
  risks and such as data loss, rendering lag, and unreliability were identified and 
  considered. This aims to ensure that testing activities address these risks and 
  validate the system's robustness against potential hazards. 
\end{itemize}
\section{Plan}

This section will go over the Verification and Validation Team members and the techniques
and methodology required for a VnV Plan's success.

\subsection{Verification and Validation Team}

\begin{enumerate}
    \item \textbf{Ethan} --- Team Lead / Test Manager
    \begin{itemize}
        \item Develop test plan, testing strategies, and test timelines
    \end{itemize}

    \item \textbf{Hussain} --- Test Analyst
    \begin{itemize}
        \item Create and review detailed tests for functional, non-functional,
         and edge cases required for the project
    \end{itemize}

    \item \textbf{Oliver} --- Quality Assurance Tester
    \begin{itemize}
        \item Conduct tests to verify that all features work as
        expected according to the SRS
    \end{itemize}

    \item \textbf{Jeffrey} --- Automation Tester
    \begin{itemize}
        \item Design, develop, and maintain most of the automated
        test scripts used for repetitive and regression testing
    \end{itemize}

    \item \textbf{Kevin} --- Document Specialist
    \begin{itemize}
        \item Write and maintain most of the documentation,
        referring to any test plans, test cases, reports, or defect logs
    \end{itemize}
\end{enumerate}


\subsection{SRS Verification}

\begin{enumerate}
\item Internal Review and Cross-Check
\begin{enumerate}
\item This is the first level of verification for the SRS. The team
will conduct a review as a whole, reviewing each member’s contribution
and giving feedback. Additionally, this will help members confirm
consistency between each individually worked on part.
\item Moreover, the team will also be given the opportunity to
cross-check with the requirements listed in any previous documents
to ensure there is consistency between documentation.
\end{enumerate}

\item Peer Review
\begin{enumerate}
\item This verification process will allow our team to seek assistance from
a peer capstone team. This portion will allow another team to verify that the
SRS is filled to completion and is error-free, ready for validation
and development.
\item Additionally, this gives our team a chance to review another capstone’s
SRS, which will allow us to cross-reference with our SRS, verifying that
both documents adhere to the Myers SRS convention and include every necessary portion.
\end{enumerate}

\item Check List Review
\begin{enumerate}
\item This level of verification will have the team review
 and cross-reference the SRS checklist once again after
 the deliverable has been completed.
\item This is an extensive SRS checklist provided by our
course instructor and is a professional way to ensure
that our SRS contains everything required of us.
\item Going over this as a team once the document has
been completed allows us to verify that all members
adhered to the checklist when creating their portion
of the SRS document.
\end{enumerate}
\end{enumerate}

\subsection{Design Verification}

\begin{enumerate}
\item Purpose:
\begin{enumerate}
\item We will use this to ensure that the design of our application
meets the specifications and user needs outlined in the SRS.
By referencing the SRS, we can verify that our application
is designed as intended and aligns with any goals or statements
made during the planning process.
\end{enumerate}

\item Design Verification Scope:
\begin{enumerate}
\item This plan will cover the verification of the following components:
\begin{enumerate}
\item User Interface (UI): Verify that the UI is intuitive
and is minimized to accommodate a more keyboard-friendly
environment, which would not require any mouse involvement.
\item Diagram Creation: Verify that the diagram creation
feature works using only a keyboard and no mouse/trackpad.
Ensure that diagrams are created with the same freedom
and range as writing on a piece of paper or using a mouse.
\item Accessibility and Usability: Verify that users
can efficiently use the application without the need
for other external tools aside from a keyboard.
\item Performance: Verify that the system works as
intended even when generating multiple objects or diagrams.
\end{enumerate}
\end{enumerate}

\item Verification Objectives:
\begin{enumerate}
\item Comply with Functional Requirements: Verify that
the design supports all key features and complies with
the functional requirements listed in all past
documents, such as the SRS.
\item Verify Performance: Verify that the
application can handle multiple diagrams
and object elements.
\end{enumerate}

\item Design Verification Methodology:
\begin{enumerate}
\item Verification through Design Reviews:
\begin{enumerate}
\item Internal Design Review:
\begin{enumerate}
\item The development team will conduct an internal
review of the application to ensure that we are
complying with all the functional requirements
specified in the SRS and other related documents.
\item The team will also verify that the
application is user-friendly by using past knowledge
on product usability and referencing
existing note-taking applications.
\end{enumerate}
\item Design Walkthrough
\begin{enumerate}
\item The development team will conduct walkthroughs
with other team members who may not have been involved
in specific feature development. Additionally,
peers and friends may be used to conduct a proper
walkthrough session.
\item During the walkthrough, ensure that users
are able to navigate the application and create
diagrams using the keyboard as intended.
\end{enumerate}
\end{enumerate}

\item Functional Testing:
\begin{enumerate}
\item Test Case Development:
\begin{enumerate}
\item Develop test cases that focus on verifying key
design features, such as keyboard navigation or
the creation of diagrams or objects using shortcuts.
\end{enumerate}
\end{enumerate}

\item Performance Testing:
\begin{enumerate}
\item Load Testing:
\begin{enumerate}
\item Perform tests that will create strain on a system
through the creation of multiple objects or diagrams
that are being manipulated. Verify that the application
is able to perform well under this load with the
system requirements specified in the SRS.
\end{enumerate}
\item Speed Test:
\begin{enumerate}
\item Verify that all interactions (i.e.,
diagram/object creation and manipulation) are
quick and responsive with minimal lag or delay.
\end{enumerate}
\end{enumerate}
\end{enumerate}

\item Design Verification Criteria:
\begin{enumerate}
\item Usability Criteria:
\begin{enumerate}
\item Users can create diagrams using only
keyboard shortcuts.
\item Users can navigate the user interface
without relying on a mouse.
\item No critical keyboard shortcut conflict,
and functionality across all supported platforms.
\end{enumerate}
\item Functional Criteria:
\begin{enumerate}
\item All diagram creation features are functional.
This includes adding shapes, manipulating elements,
connecting existing objects, and deleting components,
without the use of a mouse.
\end{enumerate}
\item Compliance with Requirements:
\begin{enumerate}
\item The design must meet all requirements outlined in the
Software Requirements Specification (SRS).
\end{enumerate}
\end{enumerate}
\end{enumerate}

\subsection{Verification and Validation Plan Verification}

\begin{enumerate}
\item Purpose:
\begin{enumerate}
\item To verify that the VnV Plan is complete, correct, and
meets the standards for verifying and validating our application.
\end{enumerate}

\item VnV Plan Verification Scope:
\begin{enumerate}
\item Completeness: Verify that all necessary sections
and objectives are included.
\item Clarity and Precision: Verify that the plan is
clearly written and unambiguous.
\item Feasibility: Verify that all techniques and
processes described to be used in the verification and
validation process are practical and achievable.
\end{enumerate}

\item VnV Plan Verification Objectives:
\begin{enumerate}
\item Verify that the VnV Plan is comprehensive and covers
all necessary portions required of a complete verification
and validation plan.
\item Verify that the techniques outlined for verification
are appropriate for validating the functional requirements
listed in the SRS.
\item Verify that the plan can realistically be executed and
will assist and result in the resolution of any defects
found in our application.
\end{enumerate}

\item Verification Techniques:
\begin{enumerate}
\item Internal Verification Plan Review:
\begin{enumerate}
\item The entire capstone team will collectively review the
document once finished. This is to verify that all members
have completed each task to a standard that the team can agree on.
\item The team will also be utilizing the checklist provided
by the course instructor. This will help in verifying that
the VnV Plan is completed correctly and incorporates all
the main features and sections required of a proper VnV Plan.
\end{enumerate}

\item Peer Review:
\begin{enumerate}
\item The VnV Plan will also be reviewed by another project
team that has also been tasked to work on a similar VnV
Plan for their capstone. This will help verify that our
team has completed the document properly by comparing
sections and content with another completed VnV document
in the same cohort.
\item Moreover, while our team also reviews the partnered
team’s VnV Plan, we can cross-reference ours with theirs to
verify that both teams have incorporated all the necessary
sections and techniques required for this deliverable.
\end{enumerate}

\item Mutation Testing:
\begin{enumerate}
\item Our team will incorporate mutation testing to verify that
our VnV Plan is working as intended. In order to do this,
we will be incorporating small changes or “mutants” and testing
to see if the verification and validation process noted would
catch them. If the plan catches these changes, it will help
confirm that the VnV plan is comprehensive and effective.
\item Example of possible mutations:
\begin{enumerate}
\item Mutation 1: Remove a key verification step
(e.g., Usability Verification) and see if the VnV
process identifies this omission.
\item Mutation 2: Add an unnecessary or incorrect
verification technique (e.g., object creation via mouse)
and see if the VnV process can identify that the new
technique is unsuitable or conflicts with other requirements.
\end{enumerate}
\end{enumerate}
\end{enumerate}
\end{enumerate}

\subsection{Implementation Verification}

\wss{You should at least point to the tests listed in this document and the unit
  testing plan.}

\wss{In this section you would also give any details of any plans for static
  verification of the implementation.  Potential techniques include code
  walkthroughs, code inspection, static analyzers, etc.}

\wss{The final class presentation in CAS 741 could be used as a code
walkthrough.  There is also a possibility of using the final presentation (in
CAS741) for a partial usability survey.}

Firstly, the tests outlined in the System Tests (and eventually the Unit Test
Description) section will be used for verification. The purpose of these tests
are to ensure the system behaves as expected given the specification, which
naturally assists in verification of the implemented system. It is expected
that not every individual unit test will be specified in the unit testing plan
and devs should not withhold from writing a useful unit test just because it is
not specified in this document. An example is writing a new unit test to cover
the expected functionality of a newly discovered bug. Devs are expected to
write tests that cover their code at the specified coverage rate of
\textit{TEST\_COVERAGE\_RATE}.

Beyond the specified test cases, the product will be test driven by members
of the team for taking notes during lectures. All members of the team are
expected to know the general expected functionality of the system, so having
the team test the product as a user would greatly benefits verifying the
product. Additionally, this is a feasible plan given the time frame and
resources of the project. All team members attend classes on a daily basis and
have the opportunity to use Flow during their lecture time.

While manual and automated dynamic tests are essential for verification of the
product, non-dynamic testing approaches can also be extremely effective and be
used throughout the development process. Typical static testing approaches
include static code analyzers, code walkthroughs, and code review. These three
approaches will be used by the team as part of the verification process.

A linting tool will be selected and each team member will be required to use it
when developing locally. To enforce its use, a new workflow step will be added
to the GitHub actions that will run the linting tool on an opened pull request.

Code review is expected to conducted on every pull request before it can be
approved and merged. To keep the plan realistic, only one review will be
required per PR, but more are encouraged if possible. In the case of looming
hard deadlines that must be met in a timely manner, the team may come to a
collective decision to skip a rigorous review process on a PR if they deem the
change to be small enough. However, the GitHub actions pipeline is still
expected to be run and pass before merging to main.

Lastly, synchronous code walkthrough may be conducted per a dev's request. This
is expected to supplement a code review if a reviewer requests it. It may also
be proposed by the code owner if they feel knowledge sharing and familiarity of
the code/architecture will be beneficial for the team. Examples include a new
module that will be reused by other devs for other parts of the codebase or
some critical functionality that would benefit from group review/verification.
Since there is more overhead with this approach due to there needing to be a
scheduled time, this is more of an option that is open to the team rather than
a strictly enforced testing plan approach.

\subsection{Automated Testing and Verification Tools}

\wss{What tools are you using for automated testing.  Likely a unit testing
  framework and maybe a profiling tool, like ValGrind.  Other possible tools
  include a static analyzer, make, continuous integration tools, test coverage
  tools, etc.  Explain your plans for summarizing code coverage metrics.
  Linters are another important class of tools.  For the programming language
  you select, you should look at the available linters.  There may also be tools
  that verify that coding standards have been respected, like flake9 for
  Python.}

\wss{If you have already done this in the development plan, you can point to
that document.}

\wss{The details of this section will likely evolve as you get closer to the
  implementation.}

Firstly, the planned tech stack for the project is an Electron app with a
React frontend using TypeScript.

For automated unit testing, we will use the \href{https://jestjs.io/}{Jest}
framework, which is a JavaScript testing framework that can be used with
TypeScript and React. For more end-to-end feature testing, we will use
\href{https://playwright.dev/}{Playwright}, which is built for end-to-end
testing web apps, but also has support for Electron apps.

For profiling and debugging, since Electron is built on top of Chromium, we can
use Chrome DevTools to measure the performance and memory usage of our app.

For static analysis and linting we will use ESLint with the TypeScript and
React plugins, which will enforce coding rules and find anti-patterns.
Additionally, Prettier will be used for formatting. The combination of these
tools will lead to a higher quality codebase with consistent styling.

Pnpm will be our build system. We can define pnpm scripts for running,
building, linting, and testing our app.

GitHub Actions will be used for continuous integration. The pipeline will
compile the app, run the automated tests, linting step, and formatting step on
each PR and main merge.

Jest has a built in feature for measuring and summarizing code test coverage,
simply by passing the `--coverage` flag. This report will then be integrated
into the CI pipeline through Coveralls, which is free for public GitHub repos
and has built in GitHub integration.

\subsection{Software Validation}

\wss{If there is any external data that can be used for validation, you should
  point to it here.  If there are no plans for validation, you should state that
  here.}

\wss{You might want to use review sessions with the stakeholder to check that
the requirements document captures the right requirements.  Maybe task based
inspection?}

\wss{For those capstone teams with an external supervisor, the Rev 0 demo should
be used as an opportunity to validate the requirements.  You should plan on
demonstrating your project to your supervisor shortly after the scheduled Rev 0 demo.
The feedback from your supervisor will be very useful for improving your project.}

\wss{For teams without an external supervisor, user testing can serve the same purpose
as a Rev 0 demo for the supervisor.}

\wss{This section might reference back to the SRS verification section.}

There are no plans for formal validation of the project as there is no formal
individual stakeholder/supervisor for the project.

As a team we are eliciting requirements amongst ourselves and we are among the
projected users of Flow, so we expect to work as a group throughout the
development process to tweak existing requirements and add missing requirements
as they are discovered.

\section{System Tests}

\wss{There should be text between all headings, even if it is just a roadmap of
the contents of the subsections.}

This section defines the system-level tests that will verify the correctness of the system’s 
functional requirements as specified in the SRS (Section S.2). Each test case focuses on validating 
user-facing behaviors and ensuring that the system performs as expected under normal and boundary conditions.

\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into
  different areas.  If there are no identifiable subsets for the tests, this
  level of document structure can be removed.}

\wss{Include a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.}

The following subsections group tests into functional areas that align with the SRS:  
1. **Editing and Canvas Operations** – covering note creation, editing, and manipulation (SRS F211–F216).  
2. **File Management and Persistence** – covering saving, loading, and export functions (SRS F217–F243).  

\subsubsection{Area of Testing1 — Editing and Canvas Operations}

\wss{It would be nice to have a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.  If a section
  covers tests for input constraints, you should reference the data constraints
  table in the SRS.}

This area tests core editing functions that define the user experience — creating, modifying, and visually 
managing elements on the canvas. Successful execution validates that the system’s interactive core behaves 
as specified in the SRS functional requirements F211–F216.

\paragraph{Note Creation and Editing}

\begin{enumerate}

\item{test-F211\\}

Control: Automatic.

Initial State: Application launched with no open document.

Input: User creates a new note and types text content.

Output: \wss{The expected result for the given inputs.  Output is not how you
are going to return the results of the test.  The output is the expected
result.}
A new note file is created and displayed in the editor window. Typed content appears 
immediately in the canvas with no delay.

Test Case Derivation: \wss{Justify the expected value given in the Output field}
The expected outcome is derived from F211 in the SRS, which specifies that users can create and edit notes in real time.  
The test verifies that note objects are correctly instantiated and rendered without performance issues.

How test will be performed: Automated UI testing via simulated input sequence that triggers note creation, typing, 
and real-time rendering validation.

\item{test-F214\\}

Control: Automatic

Initial State: Canvas open with editing mode enabled.

Input: User inserts a shape (rectangle) and moves it across the canvas.

Output: \wss{The expected result for the given inputs}
The shape appears correctly and follows the user’s movement without lag or distortion.

Test Case Derivation: \wss{Justify the expected value given in the Output field}
Expected behavior is defined by F214 and F216, requiring that the user can freely reposition and resize elements.  
The expected value is that the object’s final position and dimensions match those in the test specification.

How test will be performed: Automated functional test using simulated mouse actions and coordinate comparison of 
expected versus rendered object placement.

\end{enumerate}

\subsubsection{Area of Testing2 — File Management and Persistence}

This area validates system behavior related to file operations such as saving, loading, and exporting notes.  
It ensures compliance with SRS functional requirements F217–F243, verifying that user data is properly stored and retrieved.

\paragraph{Saving and Loading Notes}

\begin{enumerate}

\item{test-F217\\}

Control: Automatic

Initial State: A note with unsaved text and shape content exists in the editor.

Input: User selects “Save” and then “Open” from the menu.

Output: The note reopens with all previously entered text and shapes intact.

Test Case Derivation: Expected results come from SRS F217 and F241, which require data persistence and accurate file recovery.  
The test verifies successful serialization and deserialization of notes.

How test will be performed: Automated file I/O comparison between pre-save and post-load content to confirm identical state restoration.

\item{test-F243\\}

Control: Manual

Initial State: A completed note is open in the editor.

Input: User selects “Export as PDF.”

Output: A PDF file is generated containing the correct layout, formatting, and graphical content.

Test Case Derivation: Derived from F243, which specifies export functionality for interoperability.  
Expected outcome is that the exported file visually matches the on-screen content with no missing or altered elements.

How test will be performed: Manual verification by comparing the exported PDF output to the in-application rendering.

\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}

\wss{The nonfunctional requirements for accuracy will likely just reference the
  appropriate functional tests from above.  The test cases should mention
  reporting the relative error for these tests.  Not all projects will
  necessarily have nonfunctional requirements related to accuracy.}

\wss{For some nonfunctional tests, you won't be setting a target threshold for
passing the test, but rather describing the experiment you will do to measure
the quality for different inputs.  For instance, you could measure speed versus
the problem size.  The output of the test isn't pass/fail, but rather a summary
table or graph.}

\wss{Tests related to usability could include conducting a usability test and
  survey.  The survey will be in the Appendix.}

\wss{Static tests, review, inspections, and walkthroughs, will not follow the
format for the tests given below.}

\wss{If you introduce static tests in your plan, you need to provide details.
How will they be done?  In cases like code (or document) walkthroughs, who will
be involved? Be specific.}

\subsubsection{Area of Testing1}

\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Type: Functional, Dynamic, Manual, Static etc.

Initial State:

Input/Condition:

Output/Result:

How test will be performed:

\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.

Initial State:

Input:

Output:

How test will be performed:

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which
  requirements.}

\begin{table}[htbp]
\centering
\caption{Traceability Matrix for Functional Requirements}
\begin{tabular}{|p{2.5cm}|p{6cm}|p{6cm}|}
\hline
\textbf{Requirement ID} & \textbf{Requirement Description} & \textbf{Related Test(s)} \\
\hline
F211 & The system shall allow users to create and edit notes. & Test-F211: Verify that users can successfully create new notes and edit existing ones. \\
\hline
F212 & The system shall allow users to organize notes into folders and subfolders. & Test-F212: Verify that note organization functions correctly, allowing drag-and-drop or selection into designated folders. \\
\hline
F217 & The system shall allow users to view and pan across different notes and sections within the workspace. & Test-F217: Verify panning functionality and proper rendering of note sections during navigation. \\
\hline
F241 & The system shall support file management operations (save, rename, delete). & Test-F241: Confirm file management operations perform as expected, including proper updates to the note directory. \\
\hline
F242 & The system shall autosave changes at regular intervals (smaller than or equal to 60 seconds). & Test-F242: Check that autosave triggers correctly and saves the latest note content within the defined time frame. \\
\hline
\end{tabular}
\end{table}

\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}

\wss{To save space and time, it may be an option to provide less detail in this section.
For the unit tests you can potentially layout your testing strategy here.  That is, you
can explain how tests will be selected for each module.  For instance, your test building
approach could be test cases for each access program, including one test for normal behaviour
and as many tests as needed for edge cases.  Rather than create the details of the input
and output here, you could point to the unit testing code.  For this to work, you code
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}

Initial State:

Input:

Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed:

\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}

Initial State:

Input:

Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed:

\item{...\\}

\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}

Initial State:

Input/Condition:

Output/Result:

How test will be performed:

\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.

Initial State:

Input:

Output:

How test will be performed:

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}

\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.\\

\begin{description}
  \item[TEST\_COVERAGE\_RATE] - 70\%. Ideally this value would be 100\%, but 70\% was chosen as a realistic balance between time constraints and verification quality.
\end{description}

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\newpage{}
\section*{Appendix --- Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable?
  \begin{itemize}
    \item Chengze -
    \item Ethan -
    \item Hussain -
    \item Jeffrey -
    \item Kevin -
  \end{itemize}
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \begin{itemize}
    \item Chengze -
    \item Ethan -
    \item Hussain -
    \item Jeffrey -
    \item Kevin -
  \end{itemize}
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.

    Collectively, the team will need to learn a variety of skills to complete
    the verification process. Five new skills include building an automated
    testing pipeline with GitHub actions, writing unit tests for frontend
    components, writing end-to-end tests, integrating code coverage metrics,
    and performance testing/verification. Firstly, while some of the team is
    familiar with CI/CD pipelines, none of us have created and maintained one
    ourselves before. We have already tweaked our GitHub workflows for managing
    the building for our latex and adoc files, but setting up the pipeline for
    more complex steps like testing, linting, etc will require more learning.
    Secondly, while we all have experience writing unit tests in some form,
    writing tests for React and component testing will be a new concept.
    Additionally, writing end-to-end tests and setting up the infrastructure
    for running them will also be new for most of the team. Both of these
    testing approaches will involve learning to use Jest and Playwright, the
    testing frameworks we have decided to use. Some of us also have experience
    with using Coveralls for code coverage metrics, but again we will need to
    learn how to set it up ourselves. Lastly, performance testing to verify
    nonfunctional requirements is another new concept for most of the team, and
    using Chrome Dev Tools will be required to complete that.

    Generally, the team will need to collectively learn more about and improve
    test writing skills. This includes writing better software that is
    testable, which has not always been a focus in our course work. Developing
    the habit of writing tests to cover the code you write is important and
    will be new for some of us.

    As mentioned, validation is not as much of a concern for this project.

  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
  \begin{itemize}
    \item Chengze -
    \item Ethan -
    \item Hussain -
    \item Jeffrey -
    \item Kevin -
  \end{itemize}
\end{enumerate}

\end{document}
